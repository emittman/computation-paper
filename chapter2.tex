%math macros; figure out where these belong.
\newcommand{\ind}{\stackrel{ind.}{\sim}}
\newcommand{\op}{\operatorname}
\newcommand{\code}{\texttt}

\section{Introduction}
Experiments or observational studies that produce a large number of measurements for each of a small to moderate number of experimental/observational units present certain statistical challenges. Such problems are increasingly common as technologies produce more and more data. For example, gene expression profiling studies consider comparisons between experimental groups at thousands genes on the genome.

When similar comparisons are to be made between subjects for each measurement component, the number of errors is likely to be large. Because of this, there is demand for statistical methods that reduce the number of `false positives'. Shrinkage priors are often used to improve inference by borrowing information across components as a moderating influence given the limited information available for a single component. Correlations across measurement components may exist, e.g. interactions among genes, but in such scenarios as we are describing, any attempt at inference would be extremely dubious. Instead, the information being borrowed across components is with regard to between subject comparisons, such as treatment effects in the case of an experiment.

% Consider the model given by $Y_{gi}|x_i \ind F(\theta_{g}| x_i)$, $g=1,\ldots,G$, $i=1,\ldots,N$ with $G\gg N$ and the quantities of interest are functions of $\theta_g$. In this setting, there is potential benefit to borrow information across group level, $g$, to reduce variance.

A common approach is to model component-specific parameters as independent, identically distributed according to some probability distribution, $\mathcal{P}$, belonging to a parametric family of distributions. Under this framework, $\mathcal{P}$ can then be estimated allowing for conditional inference, or a prior distribution can be given on its parameters allowing for fully Bayesian inference. While this approach can be useful in practice, the influence of the choice of a particular parametric family can be considerable. This choice is often not reflective of \textit{a priori} information, but rather due to convention or convenience. Here, the choice of a `nonparametric prior', i.e. a prior distribution over a `large' set of probability distributions, allows one to avoid having to make parameteric assumptions about $\mathcal{P}$. 
% The most widely used such prior distribution in the literature of Bayesian nonparametrics is the Dirichlet process (DP).

Much effort has gone into researching methods in the case of gene expression. Several have been released as R packages \citep{edger2010,deseq2014,voom}. A common feature of these methods is that they use between-gene information to estimate the the  mean-variance relationship nonparametrically. This relationship can be key to acheiving statistical power; it involves both biological and technological sources of variation and varies from dataset to dataset \citep{voom}. Empirically, we have observed structure to be present not only between the mean and variance, but also among regression parameter components (see Fig. \ref{pairs-ind-est}). This suggests that between-gene information is available not only to regulate gene-level assessment of variance, but also the estimated effects themselves. While others used random effects models for individual regression components \citep{deseq2014,landau}], they depend on assumptions about the underlying distribution of effects, such as independence, which are contradicted by the data.

\begin{figure}[ht]
\centering
\includegraphics[width=.6\textwidth]{pairs-ind-est-std}
\caption{\small Bivariate histograms of independent estimates of effects and standard deviation obtained by ordinary least squares for 36,081 genes (data from \cite{paschold}). This example shows that random effects models assuming normality and/or independence may not be suitable for modeling gene expression hierarchically. For more on the data, including definitions of $\beta_1$, $\beta_2$, $\beta_3$, and $\sigma$, see Section \ref{sec:analysis}.}
\label{pairs-ind-est}
\end{figure}

A special case of gene expression profiling concerning two populations is differential gene expression. Because expression is typically modeled on the log scale, the differential expression at a particular gene, termed `log-fold-change', is the parameter of interest. \citet{liu} proposed a semiparametric model treating the log-fold change parameters as random effects following an unknown distribution, $\mathcal{P}$, where $\mathcal{P}$ is a random distribution arising from a Dirichlet process (DP). In this paper, we modify and extend the approach taken by \cite{liu} to allow nonparametric inference for a large class of gene profiling experiments. In our method, we use a DP to model jointly the distribution of the $p$-dimensional gene-specific regression coefficient, $\beta_g$, and variance, $\sigma^2_g$.

%The parameters of the DP either chosen, specifying a prior distribution on $\mathcal{P}$, or by placing a prior on these parameters. 
Common practice for Bayesian nonparametric applications is to sample from the joint posterior distribution using a Gibbs sampler. A number of Gibbs sampling algorithms for DP mixture models have been proposed. These can be categorized into two types: `marginal,' where full conditionals are with respect to the joint posterior with the unknown $\mathcal{P}$ integrated out, and `conditional', where $\mathcal{P}$ is also sampled and conditioned on.

When $\mathcal{P}$ is not of particular interest, marginal approaches are often preferable. However, for large $G$, these algorithms are not computationally tractable as they require one-at-a-time updating of the cluster assignment for each $g$ conditioning on all $g'\neq g$. On the other hand, conditional Gibbs methods, which depend on the ``stick-breaking" representation of the DP \citet{sethuraman}, are amenable to parallelism because cluster assignment is conditionally independent given $\mathcal{P}$. \citet{suchard} argued for wider use of graphics processing units (GPUs) by statisticians for computationally demanding, parallelizable tasks to acheive large speed-ups in real time. We provide a brief introduction to computing in the massively parallel context, including some general guidelines that can help in designing implementations that are well-suited for the GPU. We demonstrate that using a Dirichlet process mixture as a `shrinkage prior' is feasible in practice by utilizing currently available GPU technology and describe our implementation.

Section \ref{sec:data} introduces the structure of gene expression profiling data suitable for our method and recommended preprocessing steps. Section \ref{sec:model} presents our model for gene expression, which features a hierarchical model for the gene-specific parameters without parametric assumptions. Next, Section \ref{sec:inference} outlines a procedure for posterior sampling based on the blocked Gibbs sampler of \cite{ishwaran2000}. Section \ref{sec:parallel} introduces concepts related to programming for GPU parallelism and discusses details of two important parallel algorithms. The Gibbs sampler is revisited and GPU implementation details are provided. To investigate the properties of our algorithm, in Section \ref{sec:timing} we discuss a study we conducted to assess the time requirement under variable conditions. Section \ref{sec:analysis} presents an analysis of differential expression data and contrasts our results with both a gene-independent analysis and a mixed-effects analysis assuming a multivariate normal distribution on the random effects. Finally, in Section \ref{sec:discussion} we put our proposed method in context, discuss potential applications and consider future directions for research.
% In summary, to avoid sensitivity to the prior distribution on $\mathcal{P}$, we propose to model it as a truncated Dirichlet process. Advances in computers have seen a rapid increase in the use of Markov Chain Monte Carlo (MCMC) methods for fitting sophisticated statistical models. Because such methods rely on thousands or even millions of simluated draws generated one after another, fast computing is critical to making them computationally feasible. Nonparametric Bayesian methods are no different in this regard, but when the size of the data set gets moderately large, the computational effort required per draw can be enormous as the number of parameters grows with the sample size. Graphics processing units (GPUs) have potential in this area, provided that most of the work can be divided into many parallel tasks which operate on different pieces of memory. By exploiting conditional independence in the model and making use of parallelized algorithms, we demonstrate the feasibility of nonparametric Bayesian modeling in the ``big data" setting of gene expression data.
\section{Data}
\label{sec:data}
Our method is intended for cases where sample size is limited and each sample provides a high-dimensional response measurement. Such data can be represented in a tabular format, with each row associated with a component of the response and each column with an experimental/observational unit. An example is shown in Table \ref{tab:data}. We assume that a design matrix encoding the important relationships between samples is known and can be applied equally to all response components.

Important cases include RNA-seq read counts and microarray data. For these cases, the design matrix for the samples could include information about treatments, experimental design, known genotypic and phenotypic relationships. Metadata for the genes themselves are not utilized.

%TODO: Choose interesting genes

% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Thu Aug 31 13:00:59 2017
% \begin{table}[ht]
% \centering
% \label{tab:data}
% \begin{tabular}{lrrrrrrrr}
%   \hline
%  & $\mbox{B73}_1$ & $\mbox{B73}_2$ & $\mbox{B73}_3$ & $\mbox{B73}_4$ & $\mbox{Mo17}_1$ & $\mbox{Mo17}_2$ & $\mbox{Mo17}_3$ & $\mbox{Mo17}_4$ \\ 
%   \hline
% $\mbox{gene}_1$ &   1 &   0 &   0 &   2 &   0 &   1 &   0 &   0 \\ 
% $\mbox{gene}_2$ &   3 &   4 &   6 &   0 &   8 &  17 &  18 &  20 \\ 
% $\mbox{gene}_3$ & 2323 & 1533 & 1932 & 1945 & 2070 & 1582 & 2196 & 1882 \\ 
% $\mbox{gene}_4$ &   2 &   1 &   0 &   2 &   4 &   4 &   5 &   3 \\ 
% $\vdots$ &    &    &    &  $\vdots$   &   &   &   &  $\vdots$ \\ 
%    \hline
% \end{tabular}
% \end{table}
% latex table generated in R 3.4.2 by xtable 1.8-2 package
% Tue Oct 17 15:02:36 2017
\begin{table}[ht]
\centering
\caption{\small Sample of RNA-seq total read counts from \citep{paschold}. Columns names identify samples, including the genotype and a replicate number, from which we can infer the flow cell that was used for sequencing. Gene annotation information (regarding the rows), is not used for in our analysis. The first and last genes listed strongly indicate differential expression between the two inbred maize lines.}
\label{tab:data}
\vspace{.25cm}
\begin{tabular}{lrrrrrrrr}
  \hline
& $\mbox{B73}_1$ & $\mbox{B73}_2$ & $\mbox{B73}_3$ & $\mbox{B73}_4$ & $\mbox{Mo17}_1$ & $\mbox{Mo17}_2$ & $\mbox{Mo17}_3$ & $\mbox{Mo17}_4$ \\ 
  \hline
$\mbox{gene}_1$ & 666 & 590 & 654 & 703 &   3 &   3 &   1 &   1 \\ 
$\mbox{gene}_2$& 414 & 422 & 383 & 416 & 392 & 346 & 402 & 351 \\ 
  $\mbox{gene}_3$ & 1525 & 1530 & 904 & 833 & 1688 & 1568 & 1413 & 1377 \\ 
  $\mbox{gene}_4$ &  12 &  11 &   5 &   2 &   8 &  20 &   9 &   6 \\ 
  $\mbox{gene}_5$ &   1 &   1 &   0 &   2 & 951 & 945 & 1157 & 867 \\ 
   \hline
\end{tabular}
\end{table}
\iftoggle{thesis}{%
Because the read count totals are right-skewed, we use a logarithmic transformation of the RNA-seq counts, performing the analysis instead with log counts normalized by the effective library size. The details are given in Section \ref{sec:analysis}. Table \ref{tab:data-transformed} shows the result of this transformation applied to the data in Table \ref{tab:data}.
}

\section{Bayesian nonparametric hierarchical regression model}
\label{sec:model}
Let $y_{gn}$ represent the observed expression (possibly after transformation) for component $g$, sample $n$. Let $x_{n}^\top$ be the row of the design matrix $X$ corresponding to sample $n$. We will use the upper case letters $G$ and $N$, to denote the number of components and samples, respectively.
% In addition, we accommodate possible observation-level weights, $w_{gn}$. 
For the observed data, we assume the data model
\begin{equation}
\label{eq:data}
y_{gn} \ind \op{N} \left( x_{n}^\top \beta_g, \sigma^2_g \right),
\end{equation}
We model jointly

\begin{equation*}
\left(\beta_g^\top,\sigma^2_g\right) \ind \mathcal{P},
\end{equation*}
where we specify a Dirichlet process on $\mathcal{P}$, i.e.,
\begin{equation*}
\mathcal{P} \sim \op{DP}(\alpha \mbox{Q}).
\end{equation*}
\iftoggle{thesis}{
The use of this prior, due to \citet{ferguson}, is a distribution over probability distributions, such that for any finite disjoint partition $\{A_i\}_{i>=1}^n$ on $\mathbb{R}^p$, $\mathcal{P}$ is a random measure such that the joint distribution $\left(\mathcal{P}(A_1),\ldots,\mathcal{P}(A_n)\right) \sim \op{Dir}\left(\alpha Q(A_1),\ldots,\alpha Q(A_n)\right).$ The DP has two parameters: $Q$, the base measure, represents a prior guess at the distribution. $\alpha$, the concentration parameter expresses the degree to which $\mathcal{P}$ will agree with $Q$ on any set $A$. This follows from the definition given above and known properties of the Dirichlet distribution, i.e., $\op{E}\left(\mathcal{P}(A)\right)=Q(A)$, and $\op{V}\left(\mathcal{P}(A)\right)=\frac{Q(A)(1 - Q(A)}{\alpha + 1}$, showing that $\mathcal{P}(A) \stackrel{p}{\rightarrow} Q(A)$ as $\alpha \rightarrow \infty$ for any set $A$.
}{}
By modeling $\mathcal{P}$ with a DP, one can be noninformative about the overall shape of $\mathcal{P}$, allowing for irregular shapes, multimodality and so forth. An argument can be made that by incorporating our uncertainty about these features of the distribution is required for coherent interpretation of the posterior distribution \citep{walker2010bayesian}. For more information about the properties of the DP, see \cite{ferguson}.

As shown by \citet{sethuraman}, it follows from the definition of the DP that $\mathcal{P}$ is almost surely discrete and realizations of $\mathcal{P}$ can be produced by the following stick-breaking construction:

Let 
\begin{equation}
\label{eq:P}
\mathcal{P} =\sum_{k=1}^\infty \pi_k \delta_{\left(\tilde{\beta}_k^\top ,\tilde{\sigma}^2_k\right)},
\end{equation}
where $\delta_{(.)}$ is the Dirac delta function. Note that although almost sure discreteness is a property applicable to ``draws'' from a DP, the posterior for $\mathcal{P}$ is in fact a mixture of DP \citep{antoniak}. An implication of discretness can be thought of as a ``bet on sparsity"; that there are actually a finite (but unspecified) number of unique values that $(\beta_g^\top,\sigma^2_g)$ can take.  
 The ``atoms" distributed according to $Q$, specified by the product measure

\begin{equation}
\tilde{\beta}_k \ind \op{N}(m_\beta, C_\beta),\quad \tilde{\sigma}^2_k \ind \op{IG}(a_{\sigma^2}, b_{\sigma^2}),
\label{eq:Q}
\end{equation}
where ``$\op{IG}(a,b)$" refers to the inverse gamma distribution which we parameterize by shape and scale parameters, $a$ and $b$ with density given by%
\iftoggle{thesis}{
\begin{equation*}
p(x|a,b) = \frac{b^a}{\Gamma(a)}x^{a+1}e^{-b/x}.
\end{equation*}
}{
 $p(x|a,b) = \frac{b^a}{\Gamma(a)}x^{a+1}e^{-b/x}$.
}%
The mixture weights, $\pi_k$,  follow a stick-breaking process \cite{sethuraman}. Under this reparameterization,
\iftoggle{thesis}{
\begin{equation}
\nu_k = \frac{\pi_k}{1 - \sum_{l=1}^{k-1} \pi_l},
\end{equation}
}{%
$\nu_k = \frac{\pi_k}{1 - \sum_{l=1}^{k-1} \pi_l}$,%
}%
where $\nu_k$ represents the weight for cluster $k$ relative to the total probability remaining after $k-1$ breaks. For the stick-breaking construction of the DP, the $\nu_k$ are modeled by:
\iftoggle{thesis}{%
\begin{equation}
\nu_k \ind \op{Beta}(1, \alpha).
\end{equation}
}{$\nu_k \ind \op{Beta}(1, \alpha)$.}
This assumption induces a stochastically decreasing ordering of the weights, so that $\sum_{k=1}^K \pi_k \stackrel{p}{\rightarrow} 1$ as $K\rightarrow \infty$. These facts suggest that the infinite mixture distribution, $\mathcal{P}$ can be well approximated by a finite mixture. We return to this idea in Section {\ref{sec:inference}. 



\section{Model inference}
\label{sec:inference}
We adopt a Bayesian approach for estimation and inference for the DP mixture model described in Section \ref{sec:model}. This requires a specification the completion of a full probability model by specifying prior distributions. Looking at Figure \ref{dag}, which shows the directed acyclical graphs representing the model, specifying a prior amounts to choosing values for model parameters which have no arrows pointing toward them, i.e. whose distribution is not otherwise specified in the model.

We choose values of $m_\beta,\,C_\beta,\,a_{\sigma^2},\,b_{\sigma^2}$ so that $Q$ puts mass on reasonable values of the $p+1$ dimensional parameters $\left(\tilde{\beta}_g^\top,\tilde{\sigma}^2_g\right)$. First, estimates are computed for each gene independently by ordinary least squares. The hierarchical mean, $m_\beta$ is chosen so the each component, $m_{\beta,j}$, is equal to the median of the estimates for $\beta_{g,j}$. The diagonal elements of the prior covariance matrix, $C_{jj}$, are chosen to be 4 times the sample variance of the $\beta_{g,j}$. The parameters of the inverse gamma are chosen by matching first and second moments, treating the estimates for $\sigma^2_g$ as data.

This can be though of as an empirical Bayes approach. Alternatively, one could also select values based on prior experience. In any case, using informative or weakly informative priors for is important; if $Q$ is chose to be diffuse, that can be overly informative and lead to only few large $\pi_k$ accounting for most of the total probability in the mixture, $\mathcal{P}$ \citep{gelman-book}.

The parameter $\alpha$ plays an important role in the model, since it helps to determine how quickly the successive elements of $\pi$ decay, i.e. the number of clusters selected by the model. For computational convenience, we choose the conditionally conjugate prior

\begin{figure}
\includegraphics[width=.5\textwidth]{alphaprior}
\caption{\small The number of clusters determined by the model is influenced by the parameter $\alpha$, whose value determines the expected number of clusters prior to seeing the data. The figures above demonstrate the prior distribution of the prior expected number of clusters, $\op{E}(K_{occ})$ for datasets with various number of response components, $G$.}
\label{alphaprior}
\end{figure}

\begin{equation}
\alpha \sim \op{Ga}(a_\alpha,b_\alpha).
\end{equation}

This prior can be selected to express an a priori belief on the number of clusters, $K_{occ}$, in the data. \citet{escobar1994} give expression for the expected value of $K_{occ}$ given $\alpha$ and $G$ as,
\begin{equation}
\op{E}(K_{occ})=\sum_{g=1}^{G} \frac{\alpha}{\alpha + g - 1}.
\end{equation}
In that paper, the authors use a table of values derived from this formula to defend their proposal for a prior over values of $\alpha$ ranging from $G^{-1}$ to $G^{2}$, which admits values of $\op{E}(K_{occ})$ anywhere from 1 to $G$. Due to computational limitations (we require $K_{occ} \ll G$), and our prior belief, based on scientific evidence, that there are more than just a few clusters, we choose $a_\alpha=3$ and $b_\alpha=\frac{3}{G^{.5}}$ to express a prior that $\op{E}(K_{occ})$ is probably between $G^{.5}$ and $G^{.75}$ (see figure \ref{alphaprior}).

\subsection{Data augmentation and the TDP}
\label{subsec:reparam}

\begin{figure}
\includegraphics[width=.8\textwidth]{my_dag}
\caption{\small Directed acyclical graphs of the Bayesian nonparametric hierarchical regression model. Panel $(b)$ introduces latent allocation parameters, $\zeta_g$, which decouple the process which partitions the data into clusters from the distribution which provides the unique values, $(\tilde{\beta_k},\tilde{\sigma}_k^2)$ of the cluster parameters. Solid lines indicate distributional dependency, dashed lines indicate deterministic functional relationships.}
\label{dag}
\end{figure}

Panels (a) and (b) in Figure \ref{dag} show two graphical representations of the model. As explained in \cite{neal2000}, while sampling methods based on $a)$ exist, it is more efficient to decouple the process partitioning the $(\beta_g,\sigma_g)$ into clusters whose locations are realizations from the base measure, $Q$. This is done by introducing a latent variable, $\zeta_g$, taking values on the positive integers with the discrete distribution given by $Pr(\zeta_g=k)=\pi_k$. These latent variables generate a random partition of the components into clusters, with $(\beta_g,\sigma^2_g)=(\tilde{\beta}_k,\tilde{\sigma^2}_k)$ for all $g$ where $\zeta_g=k$. The expanded form of the model is shown in $b)$.

Our Gibbs sampler is based on that proposed by \citet{ishwaran2000}. In contrast to prior approaches to MCMC, detailed in \citet{neal2000}, which can be classified as ``marginal" methods, Ishwaran and Zarepour presented their blocked Gibbs sampler which allows approximate inference for DPM models. Unlike the marginal Gibbs samplers, which update the cluster allocation for each component, conditional on all other allocations, the blocked sampler jointly updates all cluster allocations independently. Being able to do so is advantageous when $G$ is large, since it becomes possible to do a large portion of the necessary computation using many parallel processes which can be executed concurrently. As we can see from Figure \ref{dag}, $\zeta_g$ are conditionally independent given $\mathcal{P}$. This is problematic since $\mathcal{P}=\sum_{k=1}^\infty \pi_k \delta_{(\beta_k,\sigma^2_k)}$ is an infinite mixture. \cite{ishwaran2001} showed that, due to the stochastic ordering of the $\pi_k$, $\mathcal{P}$ can be well-approximated by $\mathcal{P}_K=\sum_{k=1}^K \pi^*_k \delta_{(\beta_k,\sigma^2_k)}$, letting $\pi_k^*=\pi_k$ for $k<K$ and $\pi_K^* = 1-\sum_{k=1}^{K-1} \pi_k$. The authors provide an approximate lower bound to the $L_1$ distance between $\mathcal{P}_K$ and $\mathcal{P}$ as a function of $G$, $K$ and $\alpha$ and argue that because the bound decreases exponentially with $K$, it should be possible in practice to use a hierarchical model based on $\mathcal{P}_K$ that is virtually indistinguishable from one based on $\mathcal{P}$.

Applying this truncation in our model is acheived by setting $\nu_K=1$. We then have

\begin{equation*}
\zeta_g \sim \op{Cat}_K(\pi) = \sum_{k=1}^K \pi_k \delta_{k}, \mbox{ i.e. } Pr(\zeta_g=k)=\pi_k.
\end{equation*}


% In general, scientific questions will be restricted to those which can be formulated as sets of linear inequalities in $\beta_g$. 
%question: in genes with small counts, does this assumption lead to multimodality in posterior for beta_g?

\subsection{Posterior distribution}
\label{subsec:posterior}


Applying Bayes' theorem, the posterior distribution, up to a constant, is
\begin{equation*}
p(\beta,\sigma^2,\mathcal{P},\alpha|y) = p(\zeta, \tilde{\beta},\tilde{\sigma^2},\nu,\alpha | y) \propto p(y|\tilde{\beta},\tilde{\sigma^2},\zeta) p(\zeta|\nu) p(\nu|\alpha)p(\tilde{\beta},\tilde{\sigma^2})
\end{equation*}
\begin{equation*}
= \prod_{g=1}^G \left[ \op{N}(y_g;\,\beta_{\zeta_g},\sigma^2_{\zeta_g})\; \op{Cat}_K(\zeta_g;\, \pi(\nu)) \right] \prod_{k=1}^K \left[ \op{Be}(\nu_k;\, 1, \alpha)\; \op{N}(\tilde{\beta_k};\,m_\beta, C_\beta)\;\op{IG}(\tilde{\sigma}^2_k;\,a_{\sigma^2},b_{\sigma^2})\right] \op{Ga}(\alpha;a_\alpha,b_\alpha)
\end{equation*}
The first equality follows from the  invariance to reparameterization of the posterior distribution; that is, while we are interested in the posterior distribution of the $\beta_g$ and $\sigma_g$ (diagram $a)$ in Figure \ref{dag}), we lose nothing by formulating the problem in terms of $\zeta_g$, $\tilde{\beta}_k$ and $\tilde{\sigma}_k$ (diagram $b)$). The last equality follows from the product rule of conditionally independent random variables.

\subsection{Full conditionals}
\label{subsec:full-cond}
Our blocked Gibbs sampler is constructed as follows:
\paragraph{Step 1:}
Both the allocation parameters, $\zeta_g$ and the DP concentration parameter, $\alpha$, are \textit{mutually conditionally independent} given $\pi, \beta, \sigma^2$ and the full conditional distribution is given by

\begin{equation}
p(\zeta, \alpha|\cdot) \propto \prod_{g=1}^G \left[ \op{N}(y_g;\,\beta_{\zeta_g},\sigma^2_{\zeta_g}W_g)\; \op{Cat}_K(\zeta_g;\, \pi(\nu)) \right] \prod_{k=1}^K \left[ \op{Be}(\nu_k;\, 1, \alpha)\; \op{Ga}(\alpha;a_\alpha,b_\alpha)\right],
\end{equation}
with conditional independence being implied by the product rule. For $\zeta$ we get
\begin{equation}
\label{zetafull}
p(\zeta|\cdot) = \op{Cat}_K(\hat{\pi}_k), \mbox{ with}
\end{equation}
\begin{equation*}
\hat{\pi}_k \propto \pi_k \op{N}(y_g;X\tilde{\beta}_k,\tilde{\sigma}_k^2 W_g).
\end{equation*}

We see that $\alpha$ depends only on $\nu$:
\begin{equation}
    p(\alpha|\cdot) \propto \prod_{k=1}^{K-1} \op{Be}(\nu_k;1, \alpha) \; \op{Ga}(\alpha;a_\alpha,b_\alpha)
\propto \alpha^{(K-1) + a_\alpha - 1} \left(\prod_{k=1}^{K-1} (1-\nu_k)\right)^\alpha e^{-b_\alpha \alpha} 
  \end{equation}
  \begin{equation*}
    = \alpha^{(K-1) + a_\alpha - 1} e^{-(-\log \pi_K + b_\alpha) \alpha}
  \end{equation*}
  \begin{equation*}
    \implies p(\alpha|\cdot) = \op{Ga}(K + a_\alpha - 1, -\log \pi_K + b_\alpha)
  \end{equation*}



The full conditionals for $\tilde{\beta}_k$ and $\tilde{\sigma}^2_k$ are straightforward, due to conjugacy. However, due to Equation \ref{eq:Q}, the full conditional for $\tilde{\beta}_k$ depends on $\tilde{\sigma}^2_k$, and visa versa. An alternative specification for the distribution of $\tilde{\beta}_k$, $\tilde{\beta}_k \ind \op{N}\left(m_\beta, \tilde{\sigma}^2_k \mbox{diag}(c_1,\ldots,c_p)\right)$ would allow for joint updates of $(\tilde{\beta}_k,\tilde{\sigma}_k)$ with conjugacy. We opted for our approach, finding it more intuitive to specify $C_\beta$ than $\mbox{diag}(c_1,\ldots,c_p)$.

\paragraph{Step 2:} 
The full conditional for cluster location, $\tilde{\beta}_k$:
    \begin{equation}
      p(\tilde{\beta}_k|\cdot) \propto \prod_{g:\zeta_g=k} \op{N}(y_g;\,X\tilde{\beta}_k, \sigma^2_gW^{-1}_g)\,\op{N}(\tilde{\beta}_k; m, C)
    \end{equation}
    \begin{equation*}
      \implies p(\tilde{\beta}_k|\cdot) = \op{N}(\tilde{\beta}_k; \hat{m}_k,\, \hat{C}_k),
    \end{equation*}
    \begin{equation*}
    \mbox{ where }\hat{C}_k= \left( \sigma^{-2}_g\sum_{g:\zeta_g=k}
      X^\top W_g X + C^{-1} \right)^{-1}, \mbox{ and
    }\hat{m}_k=\hat{C}_k \left(\sum_{g:\zeta_g=k} X^\top W_g y_g +
      C^{-1}m \right).
    \end{equation*}
\paragraph{Step 3:} The full conditional for cluster variance, $\tilde{\sigma}_k^2$, is:
    \begin{equation}
      p(\tilde{\sigma}_k^2|\cdot) \propto \prod_{g:\zeta_g=k}
      \op{N}(y_g;\,X\tilde{\beta}_k, \sigma^2_gW^{-1}_g)
      \op{IG}(\tilde{\sigma}_k^2; a_{\sigma^2},b_{\sigma^2})
    \end{equation}
    \begin{equation*}
      \implies p(\tilde{\sigma}_k^2|\cdot) = \op{IG}(\hat{a}_k,\hat{b}_k), 
    \end{equation*}
    \begin{equation*}
      \mbox{ where }\hat{a}_k = a_{\sigma^2} + \frac{NM_k}{2},\mbox{ and }\hat{b}_k= b_{\sigma^2} + \sum_{g:\zeta_g=k}y_g^\top W_g y_g -2 \beta_g^\top X^\top W_g y_g  +\beta_g^\top X^\top W_g X \beta_g
    \end{equation*}

    Here $M_k$ is the number of $g$ for which $\zeta_g = k$. 
\paragraph{Step 4:} The full conditional for $\nu_k,\,k=1,\ldots,K-1$ depends only on $M_k(\zeta)$. The full conditional is:

  \begin{equation}
    p(\nu_k) \propto \prod_{l \ge k} \prod_{g:\zeta_g=k} \pi_k \; \op{Be}(\nu_k; 1, \alpha) \propto \nu_k^{M_k + 1}(1-\nu_k)^{\sum_{l > k} M_l + \alpha} 
  \end{equation}
  \begin{equation*}
    \implies p(\nu_k)=\op{Be}(M_k + 1, \sum_{l>k}M_l + \alpha)
  \end{equation*}

\section{Computation on the GPU}
\label{sec:parallel}
\subsection{General Remarks}
Modern GPUs offer hundreds to thousands of cores and are capable of handling over a million of concurrent processes. Compared with multi-core CPUs, which typically boast 16 or fewer processors, this suggests a large benefit to using GPUs to exploit parallelism. 
%While these benefits are becoming more widely recognized and gaining visibility through some large software projects such as Tensorflow, as with any technology there limitations, trade-offs and can be additional complications. 
We now direct attention to some differences in implementation, compared to traditional CPU programming, that are required when porting code to a GPU platform.

GPUs are usually subservient to a host CPU: the executing program is run on a CPU which turns over control periodically to the GPU to run specific tasks. These tasks, or ``kernels", follow the single instruction, multiple data (SIMD) paradigm. Each core on the GPU is assigned to work on a specific chunk of memory, but all cores execute the same program. Each instance of the program is called a ``thread". It is desirable to avoid branching logic in kernels, in part because the GPU cores are relatively slow, so branching can easily lead to high latency. The best results are obtained by having all threads proceed in lockstep.

The GPU has its own memory system. Because copying memory from host (CPU) to device (GPU) and visa versa is relatively slow, care should be taken to excessive copying from host to device and back. Ideally, necessary inputs are copied to device memory once, and any large output objects produced by the device are kept in device memory until they are complete, at which point they are copied back to the host all at once.

When programming for the CPU, memory accesses tend to be fast. On the GPU, while the thread-local (``shared") memory is fast, reading and writing from global memory is slow. Therefore, when writing kernels, one should avoid both unnecessary reading and writing from global device memory. This can often be accomplished by copying values needed more than once to variables or ``shared" arrays, for fast access. However, thread-local memory is quite limited. A good rule of thumb is to write kernels to be relatively simple, so that the memory requirements are low.

Threads are themselves organized into ``warps". When data is read from global memory, it reads not one address at a time, but in chunks to minimize overhead costs. To take advantage of this, consecutively indexed threads should use data from memory at consecutive addresses. When this happens, the reads are ``coalesced". If consecutive threads access addresses that are distant to one another, reads are not coalesced, which will make memory transfer inefficient, hence the program will tend to be slow.

For more specifics on GPU computing for statistics, with an emphasis on mixture models, see \citet{suchard}. In the following subsections, we use some examples to demonstrate some instances where parallelism can help to accelerate routine computation.

\subsection{Routines}
\label{sec:routines}
\subsubsection{Reductions}
\label{subsec:reduce}
On the GPU, individual processors are slow, and asynchronous tasks can result in many idle threads. In order to achieve speed-ups, algorithms must exploit parallelism and do so in a way that respects the limitations of the hardware. A basic example, which we use heavily, is reduction. Given some data in memory, $a_1, a_2, \ldots, a_n$, and an \textit{associative} binary operator $+$, the problem is to compute $s_{1:n}=\sum_{i=1}^n a_i$. The parallelized reduction algorithm is given below in Algorithm \label{reduce}, and is illustrated in the top half of the diagram shown in Figure \ref{scan-illustration}.  

\begin{pseudocode}[ruled]{Parallel-Reduce}{a}
\label{reduce}
\COMMENT{Parallel reduction of $n=2^D$ elements. Upon completion, $a[2^D]=s_{1:n}$}\\
\FOR d \GETS 1 \TO D \DO \BEGIN
  \FOR i \GETS 1 \TO n \mbox{ \em in parallel }\DO \BEGIN
    \IF i \mbox{ mod } 2^d=0 \DO \BEGIN
    a[i] \GETS a[i-2^{d-1}] + a[i];\\
    \END \END \END
\end{pseudocode}

\begin{figure}
\includegraphics[width=\textwidth]{diagram}
\caption{\small Diagram illustrating Parallel-Reduce and Parallel-Scan for an array of length $2^3=8$. Values in each row indicate the value after completing a step in the outer loop indicated by ``$d=x$''. $s_{i:j}$ denotes a sum of the original elements, $s_{i:j}=\sum_{k=i}^j a_k$. Arrows pointing down indicate that the previous value is carried forward from the previous step. Arrows pointing right indicate that the value in the left position increments the value previously contained in the right position. Arrows pointing left indicate that the value is copied from right to left. Parallel-Reduce results in the sum of all the original elements being contained in the right-most position. Parallel-Scan uses the partial sums produced as a side-effect of Parallel-Reduce to produce the result that each position holds the sum of the original elements to the left.}
\label{scan-illustration}
\end{figure}

To see why Parallel-Reduce works, let $a$ be an array of size $n=2^D$, $a=(a_1,\ldots,a_n)$. At the end of iteration $d$ of the outer loop, the memory in the $i$ position, where $i \mod 2^d =0$, contains the sum, $s_{(i-2^{d-1}+1):i}$, which reduces problem of computing $s_{1:n}$ to $\sum_{j=1}^{2^{D-d}}(s_{((j-1) 2^d+1):(j\cdot 2^d)})$. In the last iteration, the target is computed simply by adding the partial results in $a_{2^{D-1}}$ and $a_{2^{D}}$. Upon completion of the routine, modified positions in memory contain partial sums of the original values. These ``side effects" of algorithm turn out to be useful for another purpose, as we will see shortly. 

% Assuming no constraint on the number of processors, algorithm \ref{reduce} illustrates good GPU programming strategies. In the first iteration of the outer loop, the inner loop keeps a maximal number of threads busy with identical tasks, requiring similar and parallel memory accesses, two reads and one write. While half of these threads become idle in subsequent loops, the active threads always write to the same place. In practice, since device memory is not flat, as was discussed in the previous subsection, an optimal implementation would make use of shared memory after the first read. Reading and writing from shared memory is much faster than from global memory and is accessible to all threads in a block.

% CUDA users who want to take advantage of GPU parallelism without concerning themselves with the optimization should be aware of the Thrust library \cite{thrust}. Thrust is based on the C++ Standard Template Library and provides both containers for handling allocation and deallocation of device memory as well as generic algorithms, such as \code{thrust::reduce}.


%\caption{Parallel reduction of $n=2^d$ elements (upsweep step of scan). Upon return, total is stored in $a[n]$.}

\subsubsection{Parallel scans}
\citet{blelloch1990} described a general algorithm for parallel scans. These include parallel cumulative sums as a special case. Parallel scans can be divided into two general types: exclusive scans produce $s=(0, s_{1:1}, s_{1:2}, \ldots, s_{1:(n-1)})$ and inclusive scans produce $s=(s_{1:1},s_{1:2},\ldots,s_{1:n})$. Just as with Parallel-Reduce, `$+$' can be replaced by any associative binary operator. 
 Clearly, an exclusive scan can be generated from an exclusive scan by dropping the first element and appending $s_{1:n}$, and similarly an exclusive scan can be generated from an inclusive scan by dropping the last element and prepending a zero. While readers are likely more familiar with the inclusive scan, the parallel algorithm we discuss here is for an exclusive scan.

For simplicity, we describe the case for an array with $2^d$ elements and $2^{d-1}$ processors. The algorithm is composed of two steps: Parallel-Reduce (referred to in \cite{blelloch1990} as``upsweep"; Algorithm \ref{reduce}), which is followed by a second stage (which Blelloch refers to as ``downsweep"). Given an input array $a$, an Parallel-Reduce modifies the array $a$, defined above, so that the memory in position $i$ contains $s_{1:(i-1)}=\sum_{j=0}^{i-1}a_j$. By convention, $a_0=0$, i.e. the identity element. Pseudocode is given in Algorithm \ref{scan}.

\begin{pseudocode}[ruled]{Parallel-Scan}{a}
\label{scan}
\COMMENT{Parallel cumulative (prefix) sum of $n=2^D$ elements.}\\
a \GETS \CALL{Reduce}{a}\\
a[n] \GETS 0\\
\FOR d \GETS D \TO 1 \DO \BEGIN
  \FOR i \GETS 1 \TO n \mbox{ in parallel }\DO \BEGIN
    \IF i \mbox{ mod } 2^{d} = 0 \DO \BEGIN
    tmp \GETS a[j-2^{d-1}];\\
    a[i-2^{d-1}] \GETS a[i];\\
    a[i] \GETS tmp \oplus a[i];
    \END \END \END
\RETURN{a}
\end{pseudocode}

% A serial version of this algorithm, $z_1 = v_1,\;z_2 = z_1 \oplus v_2,\dots$, is ill-suited to the GPU.


The second stage of Parallel-Scan works with the partial results in left in memory after Parallel-Reduce. First, note that a maximum of $\lfloor \log_2(i) \rfloor$ partial sums are required for computing $s_{1:(i-1)}$, thus the number of steps in the second stage is the minimum number required. After setting the right-most position to zero, the second stage of Parallel-Scan works on the same sets of pairs of positions as Parallel-Reduce, only in reverse order. Within each pair, the value in the left position is stored in a temporary buffer, then the copy from the right position is copied to the left position. Last, the value in the right position is incremented by the value in the buffer. Intuitively, this works because the accumulated value from previous steps is a sum of values to the left of the current left position while the sum in the current left position is required by to compute the right position's target value. \citet{blelloch1990} provides a proof by induction of the correctness of the result.



% (Picking up the example of the last subsection, define $b_{i,D}b_{i,D-1}\cdots b_{i,1}b_{i,0}$ to be the binary representation of the integer $i$. For example, if $D=4$, $i=11=01011_{2}$, with $b_{i,4}=0,\, b_{i,3}=1,\,b_{i,2}=0,\,b_{i,1}=1$ and $b_{i,0}=1$. For each $b_{i,d}=1$ in $b_i$, there is a partial sum of $2^d$ values produced in the upsweep that are used in computing $s_i$. Furthermore, the number of ones is equal to the number of partial sums needed to produce the result. For example, upon completion, $a_{12}$ will have $s_{11}=\sum_{i=1}^8 v_i + \sum_{i=9}^{10}v_i+\sum_{i=11}^{11}v_i$. 
% 
% In the downsweep, at step $d$ of the outer loop, pairs of indices are formed, $i$ and $\tilde{i}$, $i>\tilde{i}$, such that $b_i$ and $b_{\tilde{i}}$ differ only in $b_{.,d}$ and both $b_{i,c}$ and $b_{\tilde{i},c}=1$ for $0\le c<d$. By construction, $i$ will have been included the previous step, while $\tilde{i}$ will have not since $b_{\tilde{i},d}=0$. Within this pairing, $a_{\tilde{i}+1}$ is set to the current value of $a_{i+1}$, i.e. the accumulation of values in the partial sums corresponding to $b_{i,D},\, \ldots, b_{i,d+1}$ while $a_{i+1}$ increments by the value in $a_{\tilde{i}+1}=s_{\tilde{i}}-s_{2^{h(\tilde{i})}}$.

% Let $S_c = \{1 \le j \le 2^{d}: j \mod 2^c = 0\},\; c=0,...,d$. Note that $S_0 \supset S_1 \supset \cdots \supset S_d$ and define $\tilde{S}_c = S_c \setminus S_{c+1}$. Then $\mathcal{C} = \{\tilde{S}_0,\ldots,\tilde{S}_d\}$ is a partition of $\mathcal{J}=\{1,\ldots,2^d-1\}$. The function $h: \mathcal{J} \rightarrow \mathcal{C}$ given by $h(j) = c$ such that $j \in \tilde{S}_c$ is one-to-one. Define $b$ to be the state of memory after performing the upsweep on $a$. Then $b[j] = \sum_{k=j - 2^h(j) + 1}^j a[k]$. For each $j$ in $\mathcal{J}$, it has a binary representation $c_{d-1} \ldots c_1 c_0$, e.g. for $d=3$ and $j=5$, we can write it as $0101$. Then, we can compute $\sum_{k=1}^{j} a[k]$ by $b[2^{d-1}c_{d-1}]+b[2^{d-2}c_{d-2}+2^{d-2}c_{d-2}]+\cdots+b[2^{d-1}c_{d-1}+\cdots+2^0c_{0}]$.
% Therefore,  compute $\sum_{k=1}^{j-1}$ for $j$ in $\mathcal{J}$, this can be done efficiently by $\sum_{c=\lfloor \log_2(j-1)\rfloor}^0 b[2^c]$

In practice, there are physical constraints that require modifications to Parallel-Scan. For example, because the size of $a$ may not be a power of 2 and also because processors are typically much less than the size of $a$, the algorithm requires modification. One solution is to break the work into chunks, each with a size that is a power of 2. Each chunk is scanned separately by a single processor (in serial) and the scan totals written to a buffer. Next, the scan totals are themselves scanned in parallel. These scanned totals can be mapped by to the chunks, and are used as offsets for the values obtained from the original scan.

Both Parallel-Reduce and Parallel-Scan presented above require $O(\log_2(n))$ steps compared to the $O(n)$ required for a serial implementation. 
%According to \citet{blelloch1990}, if $P$ processors are available, the time required to scan $n$ elements is $O(n/P + \log_2P)$. 
The actual speed-up attained depends on hardware and implementation. \citet{harris-scan} implemented Parallel-Scan for GPUs based on the implementation in \citet{blelloch1990} and observed maximum speedups of 5 times over an optimized serial CPU implementation. The advantage of the GPU implementation increased with $n$ and then leveled off, acheiving its best performance on arrays with over 1 million elements. \citet{sengupta2008} implemented a variant of Parallel-Scan that is less efficient in that it requires a larger number of operations, but reduces the number of steps from $2\log_2(n)$ to $\log_2(n)$. They reported speedups of nearly $2\times$ compared to similar implementations based on Parallel-Scan. More recently, \citet{ha2013} offered an implementation based on an algorithm that is a hybrid of the two mentioned. They reported speedups of about $1.5\times$ when compared to the implementation of \citet{sengupta2008}.

Parallel scans are used in various ways in our MCMC algorithm. For example, the calculation of cumulative probabilities to sample from categorical distributions in step 1 computes the cumulative sum -- on the log scale for numerical stability -- using $\log(e^{v_1}+e^{v_2})$ as the binary operator.
% For a thorough treatment see \citet{blelloch1990}, and for implementation details in CUDA, see \citet{harris-scan}.

% If rather than $(0,s_1,\ldots,s_{n-1}$, $(s_1,s_2,\ldots,s_n)$, the first element can be dropped and the total computed in the upsweep appended. In case that the number of threads is restricted, possible modifications have been proposed. One possibility is to perform the same cumulative sum on segments of the data, saving the totals from reduction to new array. Following that, a cumulative sum can be performed on the processor totals. Finally, using the accumulated totals as offsets, cumulative sums on the segments can be performed independently (\cite{blelloch1990}).





% Parallel scans are implemented in Thrust, called with \code{inclusive\_scan} for a full scan and \code{exclusive\_scan} for a prescan.
%\subsubsection{Pseudo-random number generation}

% Parallel uniform and normal random number generation. For the (inverse) gamma draws, the method of \citet{simplegamma} is used, pairs of which can be transformed to produce beta draws.

%\subsubsection{Linear algebra}
%For computationally intensive tasks, such as matrix multiplication, much of the work can be done independently. Because much of the work requires repeated use of the same data, however, the optimal solution depends on both the hardware and the size of the problem. Fortunately, much of this work has already been done. Just as is the case in plain C, it is recommended to use APIs to optimized, documented libraries which are being developed and made available for GPUs.
% Many LAPACK/BLAS routines, including matrix multiplication, solving lower triangular matrix equations, matrix-vector multiplication and dot product.



\subsection{Gibbs sampler revisited: exploiting parallelism}
\label{subsec:gibbs-revisited}
Owing to conjugacy, the full conditional distributions of our Gibbs sampler are known distributions and are easy to simulate from. By simulating a drawing from each full conditional per iteration of the Markov chain, the sequence of draws converge to a set of draws from the posterior distribution. There are opportunities to exploit parallelism within some of these Gibbs steps because of conditional independence among blocks of parameters. 

Insofar as the computation for these full conditionals depends on the data, it is only through summary statistics, which can be partially pre-computed (at the component level). Therefore, $y_g^\top W_g y_g$, $y_g^\top W_g X$, and $X^\top W_g X$ are computed once, prior to sampling, and saved in device memory. Also, in order to coalesce memory accesses for different steps, $\{y_g^\top W_g X\}_{1:G}$ is stored in two ways; row-wise and column-wise.

Another strategy that we use in an effort to coalesce memory accesses is to adhere to the structure of arrays ("SoA") approach \citep{thrustslides}. In order to do so, it is necessary to organize the inputs and outputs for a kernel computing $f(A_i,B_i)=C_i$ so that $A, B$ and $C$ can be accessed like a ``zipped" set of ranges with a common index, i.e. $(A,B,C)_i = (A_i,B_i,C_i)$. Because of large number of operations required, we wanted to avoid sorting and/or copying to acheive the organization required for SoA. The solution was to use fancy iterators which enable arbitrary user-specified access patterns. If, for example, we wanted to calculate $f(A_i,B_j)=C_{ij}$ for $i=1,\ldots,I$, $j=1,\ldots,J$, we can recast the problem as $f(\tilde{A}_k,\tilde{B}_k)=\tilde{C}_k$ so that $\tilde{A}_k=A_{\mbox{floor}(k/J)+1}$, $\tilde{B}_k=B_{(k+1) \mod I +1}$ and $\tilde{C}$ is a 1-d array with $IJ$ elements where $\tilde{A},\tilde{B}$ are just virtual arrays which actually just provide a map to the memory stored in $A$ and $B$. 

\paragraph{Allocation parameters}
The computational complexity for this step is $O(GKp^2)$. Fortunately, computation of the unnormalized weights is an embarassingly parallel problem. For numerical stability, these weights are computed on the log scale. From (\ref{zetafull}),
\begin{equation}
\label{logweight}
\log Pr(\zeta_g=k|\cdot) = constant + \log \pi_k -N \log \sigma_k - \frac{1}{2\sigma^2_k}\left(y_g^\top W_g y_g - 2y_g^\top W_g X \beta_k + \beta^\top X^\top W_g X \beta \right).
\end{equation}

In our implementation, first, separate kernels compute a) $y_g^\top W_g X \beta_k$ and b) $\beta^\top X^\top W_g X \beta$. a) reduces to a matrix multiplcation problem, for which optimized software has been developed for the GPU. To see this, let $B^\top D$, where $\tilde{\beta}_k$ form the columns of $B$, and $X^\top W_g y_g$ form the columns of $D$. b) is accomplished using the SoA approach. Once a) and b) have been computed and stored in global memory, (\ref{logweight}) is computed, again using an SoA approach.

To perform the categorical sampling of $\zeta$, we first perform a cumulative sum/scan of the weights (using log-sum-exp for numerical stability) for each component. Define $S_{g,k}$ to be the $k^{th}$ partial log sum for component $g$. Next, we draw $log U_g$, $U_g \sim \op{U}(0,1)$ add add the respective total log sum, $S_{g,K}$, obtaining $V_g = \log U_g + S_{g,K}$. Next, we have a kernel evaluate the comparison, $S_{g,k}<V_g$, returning $1$ if true and $0$ if false. The resulting $G\times K$ array is then reduced over each value of $g$, the result updating $\zeta$.

\paragraph{Cluster atoms}
A prerequisite to drawing from the full conditional distribution for the cluster atoms is to compute cluster summary statistics. Although this seems fairly straightforward, consisting of a series of reductions, there is an additional complication in that the values to reduce are not contiguous, as the groups being determined by the current value of $\zeta$. Rather than sorting (and copying) the data for each summary, we sort $\zeta$ storing both the sorted vector as well as the permutation that produced it. Using the stored permutation as a map for the summands, such as $y_g^\top W_g y_g$, we compute the corresponding summaries, $\sum_{g:\zeta_g=k}y_g^\top W_g y_g$, as if they were sorted using a specialized reduce algorithm, which reduces elements with a common key, the key being given by (sorted) $\zeta$. Note that this produces cluster summaries only for occupied clusters. Since the conditional distribution for cluster $k$ depends on ``updated" prior parameters that represent a combination of the data and the priors, it is convenient to fill a $K$-dimensional parameter vector, do the required computation for the occupied clusters, and then update the $K$-dimensional vector so that draws can be performed for all clusters in parallel.

\paragraph{Cluster regression coefficients}
Computation of
$\hat{C}_k^{-1}$, the precision matrix, is calculated (using AoS), followed
by Cholesky decomposition. For the sizes of $p$ we consider ($p<8$), local memory limitations do not seem to be a problem, so we simply instruct each thread to decompose one of the $K$ matrices. Next, $\hat{m}_k$ is computed
by solving
$\hat{C}_k^{-1}\hat{m}_k=\hat{C}_k^{-\frac{1}{2}}\left(\hat{C}_k^{-\frac{1}{2}}\hat{m}_k\right)
= X^\top W_g y_g$, twice using a routine for solving triangular
systems of equations. The full conditional draw for $\tilde{\beta}_k$ is
accomplished by first generating multivariate standard normal draws, $Z_k$, and setting $\tilde{\beta}_k =
\hat{C}_k^{-\frac{1}{2}} Z_k + \hat{\beta}_k$. The scaling operation is
done by solving a system of equations as before. These
computations are parallelized across clusters, $k=1,\ldots,K$.

\paragraph{Cluster variance}
For $\tilde{\sigma}^2_k$, the main hurdle is the computation of
$\hat{b}_k$. This is done in three steps, each parallel over $k$, using
the SoA approach. First,
we compute the dot products, $\beta_k^\top \sum_{g:\zeta_g=k}X^\top W_g y_g$, followed by the quadratic forms, $\beta_k^\top\left( \sum_{g:\zeta_g=k}
  X^\top W_g X \right) \beta_k$, and, lastly, parallel summation of
these with $\sum_{g:\zeta_g=k}y_g^\top y_g.$

\paragraph{Cluster weights}

The conjugate beta draws for $\nu_k$ each require two parameters. The
first shape parameters are computed by parallel elementwise addition;
the second can be computed using a parallel scan, in reverse order, of $M_k$.

\paragraph{Mass parameter}

The scalar parameter $\alpha$ depends only on the constants $K$, $a_\alpha$ and $b_\alpha$, and the scalar quantity $\pi_K$. This step can be conveniently performed by the CPU.



\subsection{Initialization}
\label{subsec:initialization}
\iftoggle{thesis}
{ \citet{papas} identified an issue with conditional samplers for DP
  mixture models that rely on the stick-breaking construction. They
  noted that, although the cluster weights are stochastically
  ordered, the ordering of cluster labels is only weakly identifiable,
  so that the posterior distributions for the weight at any given index
  is multi-modal. This is due to many configurations of the allocation
  parameters being nearly equivalent in the posterior. We suspect that, because of this weakness, poor initialization can
  lead to very slow covergence.
  % index becomes occupied, leading to a high draw for $\alpha$, which
  % increases subsequent draws for $\pi_k$, and so forth. In particular,
  % especially when $p$ is moderately large, during the initial stages the
  % $K^{th}$ cluster can become occupied. This leads to very large draws
  % for $\alpha$, which can greatly decrease the quality of the truncation
  % approximation. To avoid this problem, we reorder the cluster indices by their current value of $\pi_k$ after a short initial run, then proceed to subsequent burn-in iterations.
}

  We initialize our chains as follows. Using the ranges of independent estimates of the component-specific parameters, we set a regular $(p+1)$-dimensional grid. Using the base measure, we sort the grid values by their prior densities in descending order, retaining the first $K$. We set $\pi_k=1/K$ for all $k=1,\ldots,K$ and $\alpha$ to 1. Next we run the sampler for several thousand iterations. The initialization is taken to be the last iteration of the first chain, but with the indices reordered in descending order by $\pi$. The purpose for reordering is that, in the first chain, it is common for the last cluster to become occupied early and never become unoccupied. This is undesireable, as it undermines the quality of the truncation approximation. Reordering the indices after most of the clusters have become established seems to correct the problem (provided that $K$ is selected to be large enough.)


%Figure \ref{fig:max-id} shows traceplots of a series of Markov chains where reordering has been applied to each. The top frame shows the maximum occupied cluster ID and the bottom frame shows the value of $\alpha$. This example is simulated data with $G=40,000,\, p=6,\, \mbox{and } K=8000$.

% \begin{figure}
% \includegraphics[height=.4\textheight]{5-cycles-max-id}
% \includegraphics[height=.4\textheight]{5-cycles-alpha}
% \caption{Traceplots demonstrating high-level behavior after periodic reordering of cluster ids. Top: 5 series, maximum occupied cluster. Bottom: 5 series, $\alpha$.}
% \label{fig:max-id}
% \end{figure}


\subsection{Output}
\label{subsec:output}
Because of the dimensionality of the problem is large, it is cumbersome to save all of the MCMC samples. Following the approach in \citet{landau}, we preselect a small number of parameters for which we do save each iteration, but for the rest we first determine which functions of those parameters are of interest and use online algorithms that use running sums to update our estimates of those functions. The class of estimates we calculate fall into two categories,
\begin{enumerate}
\item expectations of the component-specific parameters and their squares, i.e. $\op{E} \beta_g,\, \op{E} \beta_g^2,\, \op{E}\sigma_g,\, \op{E}\sigma_g^2$ and 
\item component-specific posterior probabilities of conjunctions of linear
  inequalities of the elements of $\beta_g$, i.e.,
  $\op{Pr}\left(c_1\beta_g > 0 \land \ldots \land c_t\beta_g > 0\right)$.
\end{enumerate}
Updates of these quantities are embarrasingly parallel across $g$, so are well suited to the GPU. The update in 1) is done elementwise, based on a running sum, and the update in 2) can be decomposed into four tasks, a matrix multiplication to compute $C\beta$, an elementwise evaluation of the sign of the value, a parallel setwise reduction using the minimum to evaluate each conjunction, and finally an update of the estimator (using the same functionality as 1)).

We also save and return several low-dimensional parameters which provide some information on the overall behavior of the sampler: the number of occupied clusters, the maximum id of the occupied clusters and $\alpha$, the concentration parameter. We also optionally return samples of selected component-specific parameters and a user-specified number of draws of $\mathcal{P}$.


\section{Simulation Study}
\label{sec:timing}
To assess the time per iteration and time to convergence, we simulated data and sampled from the posterior using our algorithm. Seconds per iteration was calculated by dividing the total time during the sampling phase (after initialization and burn-in) by the number of sampling iterations to obtain one measure of this variable for each simulated data set. The time required to obtain a sufficient number of samples depends not only on the time per iteration, but also the number of MCMC samples required to obtain an ``effective sample" after correcting for the autocorrelation of the chain. Therefore, we also considered seconds per effective sample, where the number of effective samples was calculated in R using the `coda' package \cite{plummer}.

We identified several factors that could potentially impact the running time: the dimension of the regression coefficients, $p$, the number of groups, $G$, and truncation limit, $K$. The sample size was chosen to be proportional to the $p$, $N=4p$. Two simulated data sets were generated and analyzed for each combination of $p=2,4,6$, $G=2^12,2^13,2^14$ and $K=2^12,2^11,2^12$.

For the simulations, we first generated a distribution $P$ (as in Eq. \ref{eq:P}). We did this by first producing $K_t=\lfloor \sqrt{G}\rfloor$ cluster components, $\tilde{\beta}_k$, which were generated independently from a multivariate normal distribution with diagonal covariance $C$. We chose to have the variance decrease with the dimension, $C_{jj} = 3/j^2$. This decision was somewhat arbitrary, but is consistent with empirical observation that the proportion of significant effects is often inversely related to the dimension of the design matrix in regression. The cluster variances, $\tilde{\sigma^2}_k$ were generated independently from a gamma distribution. The component-specific coefficients, $\beta_g$ and variances, $\sigma^2_g$, were chosen by drawing an index $k$ uniformly with replacement from the set $\{1,...,K_t\}$. Finally, conditional on $(\beta_g,\sigma^2_g)$, log-expression data were generated from the normal distribution as in Equation \ref{eq:data}.

The prior distributions used to analyze the simulated data were set to the true values for $m_\beta = m$, and $C_\beta = C$. The inverse gamma parameters $a_\sigma^2$ and $b_\sigma^2$ were both set to 1. Note that $\tilde{\sigma}^2_k$ come from a gamma distribution, rather than an inverse gamma, to avoid very large values, as the inverse gamma has a much heavier tail. The prior for $\alpha$ is the one described in section \ref{sec:model}, with the prior expectation close or equal to $K_t$.depend on the independent estimates of the component-specific parameters and the number of components as described in Section \ref{sec:inference}.

For simplicity, we chose to focus on the effective number of samples for $\alpha$ because it depends on $\pi_K$, a parameter which is sensitive to changes in the number of occupied clusters and the maximum occupied cluster index. Thus, we expect that the efficiency of sampling for alpha is a reasonable proxy for the overall efficiency of the sampler.

\paragraph{Cluster parameters}
Let $K_{t} = \op{floor}(G^{0.5})$. For $k=1 \ldots K_{t}$, draw $\tilde{\beta}_k \sim \op{Normal}(0, C),\; \tilde{\sigma^2}_k \sim \op{Gamma}(1, 1)$, where $C=\op{diag}(3, 3/2^2, \ldots, 3/p^2)$.
  
\paragraph{Allocation to clusters} For $g=1,\ldots G$, draw $\zeta_g \ind \op{Discrete-Uniform}(1,K_t)$. 
  
\paragraph{Simulated data}
 Let $X = \begin{pmatrix} 1_{4 \times 1} & 0_{1\times(p-1)}\\
                                 1_{4(p-1) \times 1} & I_{(p-1)\times(p-1)} \otimes 1_{4\times 1} \end{pmatrix}$, where $\otimes$ is the Kronecker product. Draw $y_{g} \sim \op{Normal}(X\tilde{\beta}_{\zeta_g}, \tilde{\sigma}^2_{\zeta_g})$, where $y_{g}=\left(y_{g1},\ldots,y_{gN}\right)^\top$.

The left panel of Figure \ref{timing} shows the average time per iteration at each simulation setting. Note that both axes are on the log scale. As expected, there is an increase in cost associated with larger data sets. We see a substantially larger increase in running time for doubling the truncation limit, $K$, than for doubling the number of groups, $G$; this is likely because more of the computational steps depend on $K$ than depend on $G$. The cost associated with increasing the dimension of the regression, $p$, appears to be relatively small. Increases in $p$ are associated primarily with a larger amount of work required for individual threads, while increases in $G$ and $K$ incur a cost of many additional threads.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=.5\textwidth]{raw-time3}
% \caption[Average number of seconds per MCMC sample]{\scriptsize Left: Average seconds required per MCMC sample across for two simulated datasets at each level of K, the truncation limit, G, then number of genes, and p , the dimension of regression coefficient. K has the largest impact on computation time, followed by G. The effect of p is small over the range considered. Right: Average seconds per effective sample across for two simulated datasets at each level of K, the truncation limit, G, then number of genes, and p , the dimension of regression coefficient. Interestingly, the efficiency in sampling for $\alpha$ increases with p in our simulations. This might be explained by an increased separation between clusters, leading to greater stability in the partition defined by the allocation parameters.}%
% \label{raw-time}
% \end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{comb-timing}
\caption[Average number of seconds per MCMC sample]{\small Left: Average seconds required per MCMC sample across for two simulated datasets at each level of K, the truncation limit, G, then number of components, and p , the dimension of each regression coefficient, $\beta_g$. K has the largest impact on computation time, followed by G. The effect of p is small over the range considered. Right: Average seconds per effective sample across for two simulated datasets at each level of K, the truncation limit, G, then number of genes, and p , the dimension of regression coefficient. Interestingly, the efficiency in sampling for $\alpha$ increases with p in our simulations. This might be explained by an increased separation between clusters, leading to greater stability in the partition defined by the allocation parameters.}%
\label{timing}
\end{figure}

While the trends in time per iteration are fairly obvious, the results for time per effective sample are less so. This is expected, since the number of effective samples is an estimate, introducing another source of variability. The right panel of Figure \ref{timing} shows the average time per effective sample at simulation setting. While some trends reflect those seen in the raw time per iteration, there are some notable differences. The most important factor is still $K$, but $p$ has a much larger impact. Also, contrary to the previous figure, increases in $p$ are associated with a reduction in time required.

A possible explanation for this observation goes like this: In our simulation setup, the true number of clusters grows with $G$, but not with $p$. Also, the total number of samples does increase with $p$. By holding the number of clusters fixed and increasing the sample size with which to estimate their locations, while at the same time moving the clusters apart (by increasing the dimension of the parameter space), one expects that the partition of groups into clusters becomes less difficult. Since we consider the effective sample size for $\alpha$ to be a proxy for the stability of the random partition separating groups into clusters, it follows then that increases in $p$ should increase the stability of both the random partition, as well as the parameter $\alpha$.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=.9\textwidth]{eff-time2}
% \caption[Average seconds per effective sample]{Left: Average seconds per effective sample across for two simulated datasets at each level of K, the truncation limit, G, then number of genes, and p , the dimension of regression coefficient. Interestingly, the efficiency in sampling for $\alpha$ increases with p in our simulations. This might be explained by an increased separation between clusters, leading to greater stability in the partition defined by the allocation parameters.}%
% \label{eff-time}
% \end{figure}

\iftoggle{thesis}{
Figure \ref{alpha-trace} shows normalized traceplots for $\alpha$. These show shorter and less extreme excursions at higher values of $p$, which is consistent with this hypothesis.
  
  \begin{figure}[ht]
  \centering
  \includegraphics[width=.9\textwidth]{alpha-efficiency}
  \caption{Centered and scaled traceplots for $\alpha$ across levels of $p$ for various settings of $G$ and $K$.}
  \label{alpha-trace}
  \end{figure}
}

% Results of the simulation study are presented in Table \ref{tab:neff-alpha}.
% 
% % latex table generated in R 3.4.1 by xtable 1.8-2 package
% % Mon Aug 28 15:21:01 2017
% \begin{table}[ht]
% \caption{Number of seconds per effective sample for $\alpha$. Each value is computed from a 50,000 MCMC iterations (a single chain).}
% \label{tab:neff-alpha}
% \centering
% \begin{tabular}{rrrrrr}
% 
%   &   & \multicolumn{3}{c}{K}\\
%   \hline
% G & p & 1024 & 2048 & 4096 \\ 
%   \hline
% 4096  & 2 & 17 & 30 & 241 \\ 
%       & 4 & 9 & 8 & 127 \\ 
%       & 6 & 3 & 4 & 140 \\ 
% 8192  & 2 & 28 & 40 & 231 \\ 
%       & 4 & 11 & 19 & 111 \\ 
%       & 6 & 7 & 21 & 35 \\ 
% 16384 & 2 & 25 & 56 & 167 \\ 
%       & 4 & 13 & 25 & 150 \\ 
%       & 6 & 8 & 17 &  \\ 
% 32768 & 2 & 66 & 101 & 267 \\ 
%       & 4 & 6 & 46 & 113 \\ 
%       & 6 & 12 & 31 & 108 \\ 
%    \hline
% \end{tabular}
% 
% \end{table}

% \begin{figure}
% \centering
% \includegraphics[width=.9\textwidth]{neff-marginal}
% \caption{Number of effective samples for $\alpha$ marginalizing over the third factor (left:K, right:G).}
% \label{fig:neff-marginal}
% \end{figure}
% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Wed Sep 06 08:56:57 2017

Table \ref{tab:regression} shows the result of fitting a linear regression of $\log_2(\mbox{seconds}/\mbox{effective sample}$ on $\log_2(K)$, $\log_2(G)$ and $\log_2(p)$. The estimated coefficients of this model suggest that seconds per effective sample grows sublinearly in $G$, approximately quadratic in $K$ and inversely with $V$.

\begin{table}
\caption[Estimated exponentiated coefficients for the linear regression model: (model)]{\small Estimated exponentiatedcoefficients for the linear regression model: \\\hspace{\textwidth}\(\log_2$(sec. per eff. sample)$=\beta_0 + \log_2(K)\beta_1 + \log_2(G)\beta_2 + \log_2(p)\beta_3 + \epsilon\). The exponentiated values are a multiplicative effect on the predicted median seconds per effective sample for each doubling in the corresponding predictor (K,G or p).}
\vspace{0.5cm}
%   Estimated coefficients for the linear regression model,.}
\label{tab:regression}
\centering
\begin{tabular}{ccccc}
  \hline
factor &mult. effect& Est. & lower 95\% & upper 95\% \\ 
  \hline
K & $2^{\beta_1}$ & 3.3 & 2.8 & 3.8 \\ 
G & $2^{\beta_2}$ & 1.2 & 1.0 & 1.4 \\ 
p & $2^{\beta_3}$ & 0.5 & 0.4 & 0.6 \\ 
   \hline
\end{tabular}
\end{table}


\section{Example: Paschold maize data}
\label{sec:analysis}
\citet{paschold} produced an RNA-seq data set for gene expression of root tissue from 4 samples for each of two recombinant inbred maize genotypes, B73 and Mo17. Each sample was obtained from a combination of 10 primary roots from seedlings 3.5 days after germination. Illumina's Genome Analyzer II was used for sequencing. Two flow cells were utilized, with four replicates of each genotype split across flowcells. Among the researchers' aims was to identify genes which were differentially expressed in these parental lines.

\paragraph{Normalization and transformation}
For RNA-seq data, column-wise normalization (often referred to as `between-sample' normalization) is required to adjust for systematic differences arising from the sequencing process. These systematic differences obfuscate, and are irrelevant to, the quantities of interest.

For example, the total of all read counts (`library size') for a sample will vary, in part, due to differences in sequencing depth or total read count which lead to some columns having significantly higher total reads than others. A simple fix is to divide, for each sample and gene, by the library size of the sample. Unfortunately, this approach to normalization is problematic because the library size is often driven primarily by a few highly sequenced genes, making this approach too volatile. Fixes to this problem have been proposed; we used the trimmed mean of M-values (TMM) method of \citet{robinson2010} to obtain an `effective library size'. Other approaches have been proposed; for further discussion, see \citet{oshlack2010rna}.

Because the data are non-negative and variability in expression tends to increase with the overall expression,  gene expression data is typically analyzed on a logarithmic scale, with difference in expression being multiplicative. Because of the presence of zeros, we add one to each count before taking logarithms and subtracting the log of the effective library size: $y_{gn} = \log(R_{gn}+1) - \log(L_{n})$, where $R_{gn}$ is the number of reads for sample $n$, gene $g$ and $L_{n}$ is the effective library size for the $n^{th}$ sample.

% For computational reasons, we assume a normal model for the data after transforming it. First, we compute normalization factor $c_i$ for each column; second, we take logarithms of the data, after adding 1, and apply the normalization on the log scale, .i.e.
% $$\mbox{transformed log-count in column }i = \log (\mbox{original count in column }i+1) + \log(c_i)$$. 
\iftoggle{thesis}{
The result after applying these steps to the data in Table \ref{tab:data} are shown in Table \ref{tab:data-transformed}.
% latex table generated in R 3.4.2 by xtable 1.8-2 package
% Tue Oct 17 15:25:11 2017
\begin{table}[ht]
\centering
\caption{\small Same sample of RNA-seq total read counts as in Table \ref{tab:data}, after normalization and transformation. Weighted trimmed mean of M-values normalization was performed to estimate effective library size using the \texttt{calcNormFactors} function in \texttt{edgeR} \cite{edger2010}.}
\label{tab:data-transformed}
\vspace{.2in}
\centering
\begin{tabular}{lrrrrrrrr}
  \hline
& $\mbox{B73}_1$ & $\mbox{B73}_2$ & $\mbox{B73}_3$ & $\mbox{B73}_4$ & $\mbox{Mo17}_1$ & $\mbox{Mo17}_2$ & $\mbox{Mo17}_3$ & $\mbox{Mo17}_4$ \\ 
  \hline
$\mbox{gene}_1$ &-9.66 & -9.70 & -9.55 & -9.54 & -14.71 & -14.62 & -15.51 & -15.48 \\ 
$\mbox{gene}_2$ &-10.13 & -10.03 & -10.09 & -10.06 & -10.13 & -10.16 & -10.20 & -10.31 \\ 
$\mbox{gene}_3$ &-8.83 & -8.74 & -9.23 & -9.37 & -8.67 & -8.65 & -8.95 & -8.94 \\ 
$\mbox{gene}_4$ &-13.60 & -13.59 & -14.25 & -14.99 & -13.90 & -12.97 & -13.90 & -14.23 \\ 
$\mbox{gene}_5$ &-15.47 & -15.38 & -16.04 & -14.99 & -9.24 & -9.16 & -9.15 & -9.41 \\ 
   \hline
\end{tabular}
\end{table}
}{}

\subsection{Model}
Let $y_{gn}$ be the normalized log-count for gene $g$ in sample $n$. 
We assume the normal model of (\ref{eq:data}) where the design matrix for each gene is

\begin{equation*}
\begin{blockarray}{rrrr}
  Sample & \mbox{intercept} & \mbox{Mo17} & \mbox{flow cell}\\
  \begin{block}{r(rrr)}
  \mbox{B73}_1 & 1 & 0 & 0\\
  \mbox{B73}_2 & 1 & 0 & 0\\
  \mbox{B73}_3 & 1 & 0 & 1\\
  \mbox{B73}_4 & 1 & 0 & 1\\
  \mbox{Mo17}_1 & 1 & 1 & 0\\
  \mbox{Mo17}_2 & 1 & 1 & 0\\
  \mbox{Mo17}_3 & 1 & 1 & 1\\
  \mbox{Mo17}_4 & 1 & 1 & 1\\
  \end{block}
\end{blockarray}.
\label{design}
\end{equation*}

We fit the BNP model using four independent chains. Random initialization points were generated by setting unique random seeds and following the steps outlined in Section \ref{subsec:initialization}, using 1000 iterations for the initial exploration, and then reordering the cluster indices by $\pi$. 50,000 iterations were run for each chain after 10,000 iterations of burn-in. 
\iftoggle{thesis}{
Potential scale reduction factors are shown in figure \ref{rhat}. Values close to 1 are consistent with convergence; 1.1 has been suggested as a threshold \citep{gelman-book-rhat}. While some parameters exceeded 1.1, the vast majority were close to 1.
\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{rhats-param}
\caption{Gelman-Rubin potential scale reduction factors for all gene-specific parameters.}
\label{rhat}
\end{figure}
}{
To check for convergence, potential scale reduction factors (`R-hats') were estimated for all gene-specific parameters. While a few parameters had R-hats that exceeded 1.1, the vast majority were less than 1.01. Values close to 1 are consistent with convergence, while 1.1 has been suggested as a threshold \citep{gelman-book-rhat}.
}

\subsection{Comparison of methods}
In addition to our method, we also fit the data using two other methods/models: a gene independent model, fitted using least squares, and a hierarchical linear model, fitted by REML, which assumes constant variance and assumes a multivariate normal distribution for the random effects. These models were chosen to show illustrate the nonparametric shrinkage acheived by the BNP model, not because they are recommended or widely used methods for the analysis of these sorts of data.

\paragraph{Gene independent model}
Estimate $\beta_g$ in Equation \ref{eq:data} by minimizing $\sum_{n=1}^N r_{gn}^2 = \sum_{n=1}^N\left(y_{gn}-x_n\beta_g\right)^2$. This method also provides an estimate of $\sigma_g^2$, $\hat{\sigma^2}_g = \sum_{n=1}^N r_{gn}^2/(p)$.

\paragraph{Hierarchical/random effects model}
In addition to Equation \ref{eq:data}, assume $\sigma^2_g = \sigma^2$ for all $g=1,\ldots,G$, and that $\mathcal{P} = \op{N}\left(\mu, \Sigma \right)$, for some $\mu \in \mathbb{R}^p$ and $\Sigma$ a positive definite matrix. This model was fit using the $lmer$ function in the $lme4$ package in $R$ \citep{lme4,r}.

In contrast to the BNP model, the gene independent model puts no assumptions on the distribution of gene-specific parameters and borrows no information across genes. The hierarchical linear model assumes that the $\beta_g$ parameters are realizations of a multivariate normal distribution. This allows for borrowing of information, since $\mu$ and $\Sigma$ are estimated using all of the data and inference for $\beta_g$ is moderated by them.

Since the hierarchical linear model restricts $\mathcal{P}$ to the family of multivariate normal distributions, the effect of borrowing information via $\mathcal{P}$ is to pull all estimates for $\beta_g$ toward a common value. Figure \ref{P-compare} shows the empirical density of the independent estimates alongside estimates of the underlying distribution for the BNP and normal hierarchical models. These estimates are obtained by binning $10^6$ draws of the gene specific parameters -- from the posterior distribution of $P$ for BNP and from the estimate of $\mathcal{P}$ for the hierarchical model. To draw $\beta_{\mbox{new}}$ from $\mathcal{P}|y$, we simply draw $\mathcal{P}^{(s)}$ uniformly from $\{s\in 1, \ldots, S\}$ and draw $\beta_{\mbox{new}}$ from $\mathcal{P}^{(s)}$. Intuitively, by incorporating local detail in the distribution of the component-specific parameters, the BNP model can direct shrinkage more effectively than the normal hierarchical model.

In Figure \ref{pairs-3-methods}, we compare the bivariate histograms of the point estimates obtained by the hierarchical model (middle column) to the gene independent model (left column), and note that there is detectable shrinkage. This shrinkage preserves the shape of the distribution of estimates obtained by the gene independent model while reducing the magnitude of the estimates by shrinking them toward a common value. By comparison, of the point estimates obtained by the BNP model (posterior means), some are shrunk while some are not and the degree of shrinkage in many cases is more dramatic. For example, in the three-way comparison of the bottom panels, the BNP shows aggressive shrinkage, relative to the hierarchical model, of the estimates of Mo17 toward zero for those genes with the most negative flow cell effects. Figure \ref{local-shrink} shows this phenomena in greater detail. The left-panels show a zoomed in version of the lower-left plot in Figure \ref{pairs-3-methods}, with a bounding box identifying a region of interest. The next plot to the right shows the gene independent estimates from that region, while the last two plots in the row show the corresponding estimates for the hierarchical and BNP models. The BNP estimates are much less predictable, but they show clear local shrinkage toward regions with high density in the left-most plot. %The seemingly erratic behavior is related to the two additional dimensions in which the shrinkage is being applied.

\begin{figure}[ht]
\centering
\includegraphics[width=.9\textwidth]{pairs-3-methods-std}
\caption{Histogram of point estimates across genes, showing pairwise comparisons, using hexagonal binning. Relative to the independent estimates obtained by least-squares, the estimates of the normal hierarchical model shrinks all estimates toward a common mean, while the Bayesian nonparametric model shrinks estimates toward an underlying distribution learned from the data.}
\label{pairs-3-methods}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{local_shrink}
\caption{\small Comparing shrinkage in $\beta_{g2}$ and $\beta_{g3}$ under the hierarchical and BNP models. The each row shows, with a bounding box, a selected region of the parameter space. The left-most plot shows the empirical density of the gene independent estimates; these are isolated in the next plot to the right. The last plots in each row show estimates for the same genes under the hierarchical model and the BNP model, respectively. The hierarchical model shrinks monotonically and preserves most of the relative positioning of the estimates, whereas the BNP model shrinks toward nearby regions with high density (in the left-most plot).}
\label{method-compare}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{beta2v3-P-est}
\caption{\small Comparing estimates of the underlying distribution of gene specific parameters.}
\label{P-compare}
\end{figure}
% \begin{figure}
% \centering
% \includegraphics[width=.9\textwidth]{method-compare-std}
% \caption{\small Comparisons of the parameter estimates under each method.}
% \label{method-compare}
% \end{figure}
% Figure  \ref{method-compare} shows a comparison of the estimates under three models. While there is agreement about the gene-specific intercepts, there is noticable shrinkage toward the global mean for the half-difference in the linear mixed-effects model. This can be explained by the long tail of these estimates relative to a normal distribution.

Figure \ref{diff-hist} show slices in two dimensions of the joint distribution of point estimates for $\beta_g$ provided by the three models, and differences of these empirical densities. The biggest differences are in the third row, which compares the bivariate densities of point estimates of $(\beta_{g2},\beta_{g3})$ under the different models. The estimates obtained by BNP are shrunken toward two perpendicular line segments which follow the center of the distribution of the OLS estimates. The normality assumption of LMM leads to the points along the vertical axis being all shrunk monotonically toward the origin. The bottom-left plot in figure \ref{diff-hist} shows that estimates of $\sigma_g$ are clearly shrunken toward a curve which can be interpreted as an estimate of a latent mean-variance trend across genes. This is significant, as compensating for such a trend is a recognized objective in the analysis of RNA-seq data.



Figure \ref{pairs-3-methods} displays pairs of parameter estimates obtained from the different methods. The top two rows shows slight differences in the joint distribution of the gene-specific mean parameters for OLS when compared with those obtained by LMM. This difference is characterized by a shrinkage of all estimates toward the overall average, which has most noticable impact on the genes which are estimated to be far from that average.

\begin{figure}[ht]
\centering
\includegraphics[width=.65\textwidth]{mean-variance}
\caption{\small Bivariate histogram of point estimates across genes, using hexagonal binning. Relative to the independent estimates obtained by least-squares ("OLS"), the Bayesian nonparametric model shrinks estimates toward an underlying distribution learned from the data.}
\label{mean-variance}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{difference_histograms}
\caption{\small Difference of histograms of point estimates for gene-specific parameters under different models. Both BNP and LMM show shrinkage toward zero relative to OLS. The shrinkage with LMM is monotonic toward zero, while the shrinkage with BNP varies across the parameter space.}
\label{diff-hist}
\end{figure}


% Of particular interest was the identification of genes where the mean hybrid expression is higher or lower than that of both parents. For each gene, a reduced version of the design matrix (collapsing identical rows) is
% \begin{equation*}
% \begin{blockarray}{lcccc}
%   & \beta_1 & \beta_2 & \beta_3 & \beta_4\\
%   \begin{block}{l(cccc)}
%   \mbox{B73} & 1 & 1 & 0 & 0\\
%   \mbox{Mo17} & 1 & -1 & 0 & 0\\
%   \mbox{B73$\times$Mo17} & 1 & 0 & 1 & 1\\
%   \mbox{Mo17$\times$B73} & 1 & 0 & 1 & -1\\
%   \end{block}
% \end{blockarray}
% \label{design}
% \end{equation*}
% 
% In figure \ref{ggpairs_us}, histograms of the empirical distributions of the OLS estimates are shown. The irregularities seen here contraindicate the use of simple independent parametric models for the underlying parameters. In particular, we note the multimodality of $\beta_1$, the rotated `V' shape of $(\beta_2,\beta_3)$. Despite a large number of genes, we suspect that a misspecification of the model for these parameters will limit the efficacy of borrowing information across genes. We seek to improve the efficacy of borrowing information by relaxing the model to allow Bayesian learning about the underlying distribution of these parameters.
% \begin{figure}
% \includegraphics[width=\textwidth]{ggpairs_us}
% \caption{Histograms and pairwise histograms of independent OLS
%   estimates to for $(\beta_g,\log \sigma_g)$ for all genes in Paschold data set with a non-zero count.}
% \label{ggpairs_us}
% \end{figure}
% 
% Estimates of the posterior means after fitting our model are shown in figure \ref{ggpairs_s}.
% \begin{figure}
% \includegraphics[width=\textwidth]{ggpairs_s}
% \caption{Histograms and pairwise histograms of posterior means of
%   nonparametric model. Relative to the estimates in Figure
%   \ref{ggpairs_us}, there is substantial shrinkage acheived by latent
%   clustering.}
% \label{ggpairs_s}
% \end{figure}

\section{Discussion}
\label{sec:discussion}
Although quite computationally intensive, nonparametric Bayesian methods are becoming computationally feasible in applications, even for fairly sizable datasets, such as gene expression profiling. One way to achieve computational feasibility is through adapting existing procedures to take advantage of massively parallel systems like the GPU.

The large number of zero counts usually present in RNA-seq data, preclude taking logarithms of the raw data. We have circumvented the problem by adding one to the counts prior to normalization. There is theoretical support for the use of a Poisson model for RNA-seq counts absent any biological variation, a fact which is often used to motivate the use of negative binomial distribution for RNA-seq data, the negative binomial being a gamma mixture of Poissons. We opted to use a normal model for the log-frequency rather to model the counts directly. It would be possible to modify our method to use a negative binomial model for the counts and replace the  

We have found that our proposed procedure scales well and is computationally feasible with data sets containing tens of thousands of components like that which is common currently with gene expression data. In addition, design matrices up to 6 do not present a computational problem for the procedure. For larger sample sizes, we would expect that a larger number of clusters would be required to explain the data. Increasing $K$ is a computational burden; we found the time per effective iteration to be quadratic in $K$. Also, the potential benefits of borrowing information diminish with sample size, reducing the potential benefit of a hierarchical model.

\appendix
\section{CUDA libraries}
There are several libraries that make implementing these algorithms easier in CUDA. Thrust \citep{thrust} extends many concepts from the C++ Standard Template Library to Nvidia GPUs. Included are several useful generic algorithms:
\begin{itemize}
\item \code{thrust::for\_each}: This algorithm accepts a functor (an
  instance of a class with a member \code{operator()} function)
  and an iterator. The serial version increments the iterator, passing
  each element to the \code{operator()} in turn. The parallel implementation
  produces the same results, only in thread parallel fashion. The
  \code{thrust::zip\_iterator} is very useful, as it can be used
  to iterate
  over a \code{thrust::tuple}. This approach is very general, allowing
  for operations involving up to ten variables using an SoA
  design.

\item \code{thrust::reduce/reduce\_by\_key}: As described in section
  \ref{sec:parallel}, both reduce and cumulative sum are defined for any associative binary operators. Reduce by key accepts a key argument and a compatible binary predicate that identifies changes in the key. For example, for the binary predicate \code{x == y}, the key \code{\{1,1,1,2,2,1\}} would have the result be three quantities, the reductions of the first three values, the next two and the last, respectively.

\item \code{thrust::inclusive/exclusive\_scan/scan\_by\_key}: Thrust
  offers multiple versions of scan, which is another term for cumulative sum. The exclusive
  version results in the array element, $a_i$, containing the sum $s_{i-1}$ (excluding the value $v_i$, while the inclusive version leaves $a_i$ containing $s_i$.
\end{itemize}

For linear algebra on the GPU, standard installations of CUDA also
include CUBLAS \cite{cublas}. CUBLAS has implementions of BLAS/LAPACK
routines optimized for the GPU. Typically, calls to the CUBLAS
functions are initiated by the CPU and act on device memory (host
API). For newer GPUs (compute 3.5 and later), there are routines that
can be initiated within a kernel (device API). From the host API, our
algorithm uses \code{cublasDgemm} for multiplying large matrices in
device memory. From the
device API, our algorithm uses \code{cublasDtrsv} to solve many small triangular
systems of equations in parallel.

Schemes for parallel pseudo-random number generation (PRNG) have been
developed for CUDA. Since PRNGs are deterministic and sequential, a
natural parallel adaptation is access the same sequence at locations
distant enough to avoid overlap or to use a strided access
pattern. CURAND \cite{curand}, provides such functionality for generating
normal and uniform random numbers on the GPU. For other distributions,
such as gamma,
one can write a custom kernel, making use of CURAND as a source of
randomness, for threaded sampler.

