%math macros; figure out where these belong.
\newcommand{\ind}{\stackrel{ind.}{\sim}}
\newcommand{\op}{\operatorname}

\section{Model}
Let $Y_{gci}$ represent the observed expression (possibly after transformation) for gene $g$, condition $c$, replicate $i$. Let $x_{ci}^\top$ be the row of the design matrix $X$ corresponding to replicate $i$ with condition $c$. We will use the upper case letters $G$, $C$ to denote the number of genes, and conditions and let $N$, $N_c$ denote the total number of samples, and the number of replicates with condition $c$, respectively. Note that one may wish to control for other experimental or technical factors, represented as columns in the design matrix, so that $x_{ci} \neq x_{cj}$ for $i \neq j$. However, we will assume that the number of columns in $X$ is less that $N$. For the observed data, we assume the model
\begin{equation}
Y_{gci} \sim \op{N} \left( x_{ci}^\top \beta_g, \sigma^2_g \right).
\end{equation}
Next, we propose to model jointly

\begin{equation}
\left(\beta_g^\top,\sigma^2_g\right) \ind \mathcal{P},
\end{equation}
where we define

\begin{equation}
\mathcal{P} =\sum_{k=1}^\infty \pi_k \delta_{\left(\tilde{\beta}_k^\top ,\tilde{\sigma}^2_k\right)}.
\end{equation}
Here $\delta_{(.)}$ is the Dirac delta function. The so-called ``atoms" of this infinite mixture of degenerate distributions are themselves modeled as random variables with the distribution given by

\begin{equation}
\tilde{\beta}_k \ind \op{N}(\eta, \tau^2),\quad \tilde{\sigma}_k \ind \op{IG}(\gamma, \lambda).
\end{equation}
The mixture weights, $\pi_k$,  are assumed to follow a stick-breaking process introduced by \citet{sethuraman}. Using the reparameterization,

\begin{equation}
\nu_k = \frac{\pi_k}{1 - \sum_{l=1}^{k-1} \pi_l},
\end{equation}

$\nu_k$ representing a proportion of the total probability remaining after $k-1$ breaks, we assume
\begin{equation}
\nu_k \ind \op{Beta}(1, \alpha),
\end{equation}

inducing a stochastic ordering of the weights. The parameter $\alpha$ plays an important role in the model, as it controls the rate of decay in these weights. In order to allow the data to aid in determining an appropriate value, we assume the following prior on $\alpha$:
\begin{equation}
\alpha \sim \op{Gamma}(\Gamma, \Omega).
\end{equation}
%question: in genes with small counts, does this assumption lead to multimodality in posterior for beta_g?


% \[Y_{g}|Z_g \sim N(X\beta_{z[g]}, \sigma^2)\]
% \[Z_g \stackrel{iid}{\sim} Categorical(K, \pi)\]
% \[\beta_k \stackrel{iid}{\sim} N(0,1)\]
% \[p(\sigma^2) \propto \frac{1}{\sigma^2}\]
% \[\pi \sim stick(1)\]
% 
% 
% \begin{document}
% 
% \section{Definitions}
% \[ G = \mbox{number of genes}\]
% \[K = \mbox{number of clusters},\]
% \[V = \mbox{number of varieties/treatments},\]
% \[N = \mbox{number of replicates(variety)}\]
% \[y_g = \mbox{gene expression measurements for gene g (V*N vector)}\]
% \[X = \mbox{design matrix (same for all genes)}\]
% \section{MCMC}
% 
% \subsection{z step:}
% \[Z_g|y_g,\beta,\sigma^2,\pi \sim \operatorname{Categorical}(K, \pi_{g})\]
% \[\pi_{gk} \propto \pi_k\exp\left\{(\beta_k^TX^Ty_g - \frac{1}{2}\beta_k^TX^TX\beta_k)/\sigma^2\right\}, \mbox{ with } \sum_{k=1}^K\pi_{gk} = 1\]
% 
% \subsection{$\theta$ step:}
% \[\beta_k|\cdots \sim \operatorname{Normal}(\hat{\beta}_k, \sigma^2 V_k),\]
% \[\sigma^2|\xout{\beta} \cdots \sim \operatorname{Inv-Gamma}(a+GVN/2, b+(y^Ty-\sum_k \hat{\beta_k}^T V_k^{-1}\hat{\beta_k})/2),\]
% where
% \[G_k=\#\{g:z_g=k\}\]
% \[\hat{\beta}_k = (G_kX^TX+\lambda I)^{-1}\sum_{g:z_g=k}X^Ty_g,\]
% \[V_k = (G_k X^TX+\lambda I)^{-1}\]
% 
% \subsection{$\pi$ step:}
% \[\pi_k = \begin{cases} V_1 & k=1\\ V_k\prod_{j=1}^{k-1}(1-V_{k-1}) & 1<k<K \\
% \prod_{j=1}^{k-1}(1-V_{j}) & k=K \end{cases}\]
% \[V_k \stackrel{i.i.d}{\sim} \operatorname{Beta}(1 + G_k, \alpha + H_k),\]
% \[H_k = \sum_{l+1}^{K} G_l\]
% 
% \section{Derivations}
% \[\{\beta_g\},\sigma^2 | y, z \propto \frac{1}{\sigma^{2}} \prod_{k=1}^{K} \left[ \frac{1}{\sigma} \exp\left\{ \frac{-\lambda}{2\sigma^2} \beta_k^\top\beta_k \right\}\prod_{\{g:z_g=k\}}\frac{1}{\sigma^{VN}}\exp \left\{ \frac{-1}{2\sigma^2}(y_g-X\beta_k)^\top(y_g-X\beta_k) \right\} \right] \]
% \[=\frac{1}{\sigma^{GVN+K+2}}\prod_{k=1}^{K} \exp\left\{\frac{-1}{2\sigma^2}\left[ \sum_{\{g:z_g=k\}} y_g^\top y_g + \beta_K^\top(X^\top X+\lambda I)\beta_k  - 2\sum_{\{g:z_g=k\}}y_g^\top X \beta_k \right] \right\}\]
% \[=\frac{1}{\sigma^{GVN+K+2}}\prod_{k=1}^{K} \exp \left\{\frac{-1}{2\sigma^2}\left[ \sum_{\{g:z_g=k\}} y_g^\top y_g + (\beta_k-\hat{\beta}_k)^\top (X^\top X+\lambda I)(\beta_k-\hat{\beta}_k)  -  \hat{\beta}_k^\top(X^\top X + \lambda I)\hat{\beta}_k \right] \right\}\]
% \[=\frac{1}{\sigma^{GVN + 2}}\exp\left\{\frac{-1}{\sigma^2}\left[ \frac{1}{2} \left( y^\top y  -  \sum_{k=1}^{K}\hat{\beta}_k^\top(X^\top X + \lambda I)\hat{\beta}_k \right) \right]\right\} \cdot\]
% \[\prod_{k=1}^{K}\frac{1}{\sigma}\exp\left\{\frac{-1}{2\sigma^2}\left[(\beta_k-\hat{\beta}_k)^\top (X^\top X+\lambda I)(\beta_k-\hat{\beta}_k) \right]\right\},\]
% where $\hat{\beta}_k = (X^\top X + \lambda I)^{-1}\sum_{\{g:z_g=k\}} X^\top y_g$.
% 
% \section{Preliminaries}
% The Blocked Gibbs sampler targeting the joint posterior for this model is very computationally demanding. For every iteration, the z step requires computing and writing to memory $G\cdot K$ likelihoods, performing $G$ cumulative sums (scans) of $K$ reals, $G\cdot K$ logical comparisons and $G$ reductions of $K$ reals. However, conditional independence in the $z_g$'s allows for embarassingly parallel computation over the $G$ genes. This is what we plan to exploit with a GPU.
% 
% Precalculating $X^Ty$ (or $y^TX$) for each gene can be done once and stored in global memory.

\end{document}

\section{Posterior distribution}